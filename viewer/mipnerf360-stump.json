{
  "state": {
    "dataset_info": {
      "id": "mipnerf360",
      "color_space": "srgb",
      "evaluation_protocol": "nerf",
      "scene": "stump",
      "downscale_factor": 4,
      "type": "object-centric",
      "expected_scene_scale": 4.188471412658691,
      "name": "Mip-NeRF 360",
      "description": "Mip-NeRF 360 is a collection of four indoor and five outdoor object-centric scenes. The camera trajectory is an orbit around the object with fixed elevation and radius. The test set takes each n-th frame of the trajectory as test views.",
      "paper_title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
      "paper_authors": [
        "Jonathan T. Barron",
        "Ben Mildenhall",
        "Dor Verbin",
        "Pratul P. Srinivasan",
        "Peter Hedman"
      ],
      "paper_link": "https://arxiv.org/pdf/2111.12077.pdf",
      "link": "https://jonbarron.info/mipnerf360/",
      "metrics": [
        {
          "id": "psnr",
          "name": "PSNR",
          "description": "Peak Signal to Noise Ratio. The higher the better.",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"
        },
        {
          "id": "ssim",
          "name": "SSIM",
          "description": "Structural Similarity Index. The higher the better. The implementation matches JAX's SSIM and torchmetrics's SSIM (with default parameters).",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Structural_similarity"
        },
        {
          "id": "lpips_vgg",
          "name": "LPIPS (VGG)",
          "description": "Learned Perceptual Image Patch Similarity. The lower the better. The implementation uses VGG backbone and matches lpips pip package with checkpoint version 0.1",
          "ascending": false,
          "link": "https://richzhang.github.io/PerceptualSimilarity/"
        }
      ],
      "default_metric": "psnr",
      "scenes": [
        {
          "id": "garden",
          "name": "garden"
        },
        {
          "id": "bicycle",
          "name": "bicycle"
        },
        {
          "id": "flowers",
          "name": "flowers"
        },
        {
          "id": "treehill",
          "name": "treehill"
        },
        {
          "id": "stump",
          "name": "stump"
        },
        {
          "id": "kitchen",
          "name": "kitchen"
        },
        {
          "id": "bonsai",
          "name": "bonsai"
        },
        {
          "id": "counter",
          "name": "counter"
        },
        {
          "id": "room",
          "name": "room"
        }
      ]
    }
  },
  "viewer_transform": [
    0.15479412523404207,
    0.13592330631009028,
    -0.12038191246434035,
    0.009754612474239949,
    0.1815283364572884,
    -0.11254766701591598,
    0.10634215385155395,
    -0.10444329547909094,
    0.0037958417995670787,
    -0.1605803475449247,
    -0.17643047473321727,
    0.23584158579250009
  ],
  "viewer_initial_pose": [
    0.8003846564933853,
    0.03992720239005722,
    -0.5981557036131206,
    0.5917947536691911,
    0.5989919661228531,
    -0.09379411875551064,
    0.7952428921546283,
    -0.7149200015089064,
    -0.02435167014499559,
    -0.9947906720174758,
    -0.09898746301614442,
    -0.1782950953256095
  ],
  "dataset": {
    "url": "https://huggingface.co/datasets/nerfbaselines/nerfbaselines-data/resolve/main/mipnerf360/stump-nbv.json"
  }
}