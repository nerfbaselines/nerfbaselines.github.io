{
  "state": {
    "dataset_info": {
      "id": "mipnerf360",
      "color_space": "srgb",
      "evaluation_protocol": "nerf",
      "scene": "room",
      "downscale_factor": 2,
      "type": "object-centric",
      "expected_scene_scale": 5.41783709526062,
      "name": "Mip-NeRF 360",
      "description": "Mip-NeRF 360 is a collection of four indoor and five outdoor object-centric scenes. The camera trajectory is an orbit around the object with fixed elevation and radius. The test set takes each n-th frame of the trajectory as test views.",
      "paper_title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
      "paper_authors": [
        "Jonathan T. Barron",
        "Ben Mildenhall",
        "Dor Verbin",
        "Pratul P. Srinivasan",
        "Peter Hedman"
      ],
      "paper_link": "https://arxiv.org/pdf/2111.12077.pdf",
      "link": "https://jonbarron.info/mipnerf360/",
      "metrics": [
        {
          "id": "psnr",
          "name": "PSNR",
          "description": "Peak Signal to Noise Ratio. The higher the better.",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"
        },
        {
          "id": "ssim",
          "name": "SSIM",
          "description": "Structural Similarity Index. The higher the better. The implementation matches JAX's SSIM and torchmetrics's SSIM (with default parameters).",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Structural_similarity"
        },
        {
          "id": "lpips_vgg",
          "name": "LPIPS (VGG)",
          "description": "Learned Perceptual Image Patch Similarity. The lower the better. The implementation uses VGG backbone and matches lpips pip package with checkpoint version 0.1",
          "ascending": false,
          "link": "https://richzhang.github.io/PerceptualSimilarity/"
        }
      ],
      "default_metric": "psnr",
      "scenes": [
        {
          "id": "garden",
          "name": "garden"
        },
        {
          "id": "bicycle",
          "name": "bicycle"
        },
        {
          "id": "flowers",
          "name": "flowers"
        },
        {
          "id": "treehill",
          "name": "treehill"
        },
        {
          "id": "stump",
          "name": "stump"
        },
        {
          "id": "kitchen",
          "name": "kitchen"
        },
        {
          "id": "bonsai",
          "name": "bonsai"
        },
        {
          "id": "counter",
          "name": "counter"
        },
        {
          "id": "room",
          "name": "room"
        }
      ]
    },
    "render_resolution": 768,
    "prerender_enabled": false,
    "method_info": {
      "method_id": "gsplat",
      "hparams": null,
      "supported_camera_models": [
        "pinhole"
      ],
      "supported_outputs": [
        "accumulation",
        "color",
        "depth"
      ],
      "name": "gsplat",
      "description": "gsplat is an open-source library for CUDA accelerated rasterization of gaussians with python bindings. It is inspired by the 3DGS paper, but it is faster, more memory efficient, and with a growing list of new features. In NerfBaselines, the method was modified to enable appearance optimization, to support sampling masks, and to support setting background color (which is required for the Blender dataset).",
      "paper_title": "gsplat: An Open-Source Library for Gaussian Splatting",
      "paper_authors": [
        "Vickie Ye",
        "Ruilong Li",
        "Justin Kerr",
        "Matias Turkulainen",
        "Brent Yi",
        "Zhuoyang Pan",
        "Otto Seiskari",
        "Jianbo Ye",
        "Jeffrey Hu",
        "Matthew Tancik",
        "Angjoo Kanazawa"
      ],
      "paper_link": "https://arxiv.org/pdf/2409.06765.pdf",
      "link": "https://docs.gsplat.studio/main/",
      "licenses": [
        {
          "name": "Apache 2.0",
          "url": "https://raw.githubusercontent.com/nerfstudio-project/gsplat/main/LICENSE"
        }
      ],
      "nb_version": "1.1.1.dev6+g5df36ec.d20240910",
      "num_iterations": 30000,
      "total_train_time": 1709.01893,
      "resources_utilization": {
        "memory": 2537,
        "gpu_name": "NVIDIA A100-SXM4-40GB",
        "gpu_memory": 6256
      },
      "datetime": "2024-09-11T16:41:29",
      "config_overrides": {},
      "applied_presets": [],
      "dataset_metadata": {
        "id": "mipnerf360",
        "color_space": "srgb",
        "evaluation_protocol": "nerf",
        "scene": "room",
        "downscale_factor": 2,
        "type": "object-centric",
        "viewer_transform": [
          [
            0.021615,
            -0.080797,
            0.155217,
            0.086591
          ],
          [
            -0.174913,
            -0.00542,
            0.021537,
            0.062656
          ],
          [
            -0.005098,
            -0.156621,
            -0.080818,
            0.276204
          ]
        ],
        "viewer_initial_pose": [
          [
            -0.418503,
            -0.359906,
            0.83386,
            -0.612841
          ],
          [
            -0.908144,
            0.154313,
            -0.389181,
            0.404165
          ],
          [
            0.011393,
            -0.920139,
            -0.391427,
            0.265555
          ]
        ],
        "expected_scene_scale": 5.417837
      },
      "evaluation_protocol": "nerf",
      "checkpoint_sha256": "dff65893e2b59c9a3cbe02ef0bd5ab94aa801941b85a73d73336b979b588c52f"
    },
    "outputs_configuration": {
      "color": {},
      "depth": {
        "palette_enabled": true
      },
      "accumulation": {
        "palette_enabled": true,
        "range_min": 0,
        "range_max": 1
      }
    }
  },
  "viewer_transform": [
    0.02161545094088316,
    -0.08079698960965041,
    0.15521716368048286,
    0.08659090766947489,
    -0.17491292780479237,
    -0.005419941342721438,
    0.02153696600235412,
    0.06265604602802297,
    -0.005097938310461649,
    -0.1566212617924149,
    -0.08081794519989922,
    0.2762035288339131
  ],
  "viewer_initial_pose": [
    -0.4185028061575934,
    -0.3599059031442802,
    0.8338604016562333,
    -0.6128405412626861,
    -0.9081440253868295,
    0.15431275229312047,
    -0.38918117507952305,
    0.40416453978937594,
    0.01139330122348635,
    -0.920138751277078,
    -0.3914267242016044,
    0.26555527816312496
  ],
  "method_id": "gsplat",
  "renderer": {
    "transform": [
      0.04846677545237364,
      4.360622025101928,
      0.6680350303539305,
      0.3311359079410856,
      -1.9203227419374842,
      -0.5806043485297524,
      3.9292370039452695,
      2.6749753785464523,
      3.9716062008187194,
      -0.33394373863132876,
      1.8916844453962136,
      0.5329852699061767
    ],
    "scene_url": "https://huggingface.co/datasets/nerfbaselines/nerfbaselines-supplementary/resolve/main/gsplat/mipnerf360/room_demo/scene.ksplat",
    "type": "3dgs"
  },
  "dataset": {
    "url": "https://huggingface.co/datasets/nerfbaselines/nerfbaselines-data/resolve/main/mipnerf360/room-nbv.json"
  }
}