{
  "state": {
    "dataset_info": {
      "id": "mipnerf360-sparse",
      "color_space": "srgb",
      "evaluation_protocol": "nerf",
      "scene": "counter-n12",
      "downscale_factor": 2,
      "type": "object-centric",
      "expected_scene_scale": 4.4187685489654545,
      "name": "Mip-NeRF 360 Sparse",
      "description": "Modified Mip-NeRF 360 dataset with small train set (12 or 24) views. The dataset is used to evaluate sparse-view NVS methods.",
      "paper_title": "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields",
      "paper_authors": [
        "Jonathan T. Barron",
        "Ben Mildenhall",
        "Dor Verbin",
        "Pratul P. Srinivasan",
        "Peter Hedman"
      ],
      "paper_link": "https://arxiv.org/pdf/2111.12077.pdf",
      "link": "https://jonbarron.info/mipnerf360/",
      "metrics": [
        {
          "id": "psnr",
          "name": "PSNR",
          "description": "Peak Signal to Noise Ratio. The higher the better.",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"
        },
        {
          "id": "ssim",
          "name": "SSIM",
          "description": "Structural Similarity Index. The higher the better. The implementation matches JAX's SSIM and torchmetrics's SSIM (with default parameters).",
          "ascending": true,
          "link": "https://en.wikipedia.org/wiki/Structural_similarity"
        },
        {
          "id": "lpips_vgg",
          "name": "LPIPS (VGG)",
          "description": "Learned Perceptual Image Patch Similarity. The lower the better. The implementation uses VGG backbone and matches lpips pip package with checkpoint version 0.1",
          "ascending": false,
          "link": "https://richzhang.github.io/PerceptualSimilarity/"
        }
      ],
      "default_metric": "psnr",
      "scenes": [
        {
          "id": "garden-n12",
          "name": "garden n12"
        },
        {
          "id": "bicycle-n12",
          "name": "bicycle n12"
        },
        {
          "id": "flowers-n12",
          "name": "flowers n12"
        },
        {
          "id": "treehill-n12",
          "name": "treehill n12"
        },
        {
          "id": "stump-n12",
          "name": "stump n12"
        },
        {
          "id": "kitchen-n12",
          "name": "kitchen n12"
        },
        {
          "id": "bonsai-n12",
          "name": "bonsai n12"
        },
        {
          "id": "counter-n12",
          "name": "counter n12"
        },
        {
          "id": "room-n12",
          "name": "room n12"
        },
        {
          "id": "garden-n24",
          "name": "garden n24"
        },
        {
          "id": "bicycle-n24",
          "name": "bicycle n24"
        },
        {
          "id": "flowers-n24",
          "name": "flowers n24"
        },
        {
          "id": "treehill-n24",
          "name": "treehill n24"
        },
        {
          "id": "stump-n24",
          "name": "stump n24"
        },
        {
          "id": "kitchen-n24",
          "name": "kitchen n24"
        },
        {
          "id": "bonsai-n24",
          "name": "bonsai n24"
        },
        {
          "id": "counter-n24",
          "name": "counter n24"
        },
        {
          "id": "room-n24",
          "name": "room n24"
        }
      ]
    },
    "render_resolution": 768,
    "prerender_enabled": false,
    "method_info": {
      "method_id": "scaffold-gs",
      "hparams": {
        "sh_degree": 3,
        "feat_dim": 32,
        "n_offsets": 10,
        "voxel_size": 0.001,
        "update_depth": 3,
        "update_init_factor": 16,
        "update_hierachy_factor": 4,
        "use_feat_bank": false,
        "white_background": false,
        "lod": 0,
        "appearance_dim": 0,
        "lowpoly": false,
        "ds": 1,
        "ratio": 1,
        "undistorted": false,
        "add_opacity_dist": false,
        "add_cov_dist": false,
        "add_color_dist": false,
        "scale_coords": null,
        "iterations": 30000,
        "position_lr_init": 0.0,
        "position_lr_final": 0.0,
        "position_lr_delay_mult": 0.01,
        "position_lr_max_steps": 30000,
        "offset_lr_init": 0.01,
        "offset_lr_final": 0.0001,
        "offset_lr_delay_mult": 0.01,
        "offset_lr_max_steps": 30000,
        "feature_lr": 0.0075,
        "opacity_lr": 0.02,
        "scaling_lr": 0.007,
        "rotation_lr": 0.002,
        "mlp_opacity_lr_init": 0.002,
        "mlp_opacity_lr_final": 2e-05,
        "mlp_opacity_lr_delay_mult": 0.01,
        "mlp_opacity_lr_max_steps": 30000,
        "mlp_cov_lr_init": 0.004,
        "mlp_cov_lr_final": 0.004,
        "mlp_cov_lr_delay_mult": 0.01,
        "mlp_cov_lr_max_steps": 30000,
        "mlp_color_lr_init": 0.008,
        "mlp_color_lr_final": 5e-05,
        "mlp_color_lr_delay_mult": 0.01,
        "mlp_color_lr_max_steps": 30000,
        "mlp_featurebank_lr_init": 0.01,
        "mlp_featurebank_lr_final": 1e-05,
        "mlp_featurebank_lr_delay_mult": 0.01,
        "mlp_featurebank_lr_max_steps": 30000,
        "appearance_lr_init": 0.05,
        "appearance_lr_final": 0.0005,
        "appearance_lr_delay_mult": 0.01,
        "appearance_lr_max_steps": 30000,
        "percent_dense": 0.01,
        "lambda_dssim": 0.2,
        "start_stat": 500,
        "update_from": 1500,
        "update_interval": 100,
        "update_until": 15000,
        "min_opacity": 0.005,
        "success_threshold": 0.8,
        "densify_grad_threshold": 0.0002,
        "test_optim_lr": 0.1,
        "test_optim_steps": 128,
        "convert_SHs_python": false,
        "compute_cov3D_python": false,
        "debug": false
      },
      "supported_camera_models": [
        "pinhole"
      ],
      "supported_outputs": [
        "color"
      ],
      "name": "Scaffold-GS",
      "description": "Scaffold-GS uses anchor points to distribute local 3D Gaussians, and predicts their attributes on-the-fly based on viewing direction and distance within the view frustum. In NerfBaselines, we fixed bug with cx,cy, added appearance embedding optimization, and added support for masks. Note, that we also implement a demo for the method, but it does not evaluate the MLP and the Gaussians are \"baked\" for specific viewing direction and appearance embedding (if enabled).",
      "paper_title": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering",
      "paper_authors": [
        "Tao Lu",
        "Mulin Yu",
        "Linning Xu",
        "Yuanbo Xiangli",
        "Limin Wang",
        "Dahua LinBo Dai"
      ],
      "paper_link": "https://arxiv.org/pdf/2312.00109.pdf",
      "link": "https://city-super.github.io/scaffold-gs/",
      "licenses": [
        {
          "name": "custom, research only",
          "url": "https://raw.githubusercontent.com/city-super/Scaffold-GS/main/LICENSE.md"
        }
      ],
      "nb_version": "1.2.9.dev0+g09eaa2c.d20250222",
      "num_iterations": 30000,
      "total_train_time": 1386.02791,
      "resources_utilization": {
        "memory": 3236,
        "gpu_name": "NVIDIA A100-SXM4-40GB",
        "gpu_memory": 6124
      },
      "datetime": "2025-07-29T20:24:12",
      "config_overrides": {
        "voxel_size": 0.001,
        "update_init_factor": 16,
        "appearance_dim": 0,
        "ratio": 1
      },
      "applied_presets": [
        "mipnerf360"
      ],
      "dataset_metadata": {
        "id": "mipnerf360-sparse",
        "color_space": "srgb",
        "evaluation_protocol": "nerf",
        "scene": "counter-n12",
        "downscale_factor": 2,
        "type": "object-centric",
        "viewer_transform": [
          -0.069623,
          -0.108469,
          0.177397,
          -0.214747,
          -0.206608,
          0.01502,
          -0.071904,
          0.204313,
          0.023417,
          -0.189978,
          -0.10697,
          0.557778
        ],
        "viewer_initial_pose": [
          0.63229,
          -0.361699,
          0.685116,
          -0.699069,
          -0.763028,
          -0.443873,
          0.469857,
          -0.314454,
          0.134158,
          -0.819848,
          -0.556643,
          0.429672
        ],
        "expected_scene_scale": 4.418769
      },
      "evaluation_protocol": "nerf",
      "checkpoint_sha256": "a8a516a3694ede5f89b0d496102649abdb2e88e7e9f8af09ab44383095b33cc9"
    },
    "outputs_configuration": {
      "color": {}
    }
  },
  "viewer_transform": [
    -0.06962345251518143,
    -0.10846861191354908,
    0.1773972913215211,
    -0.21474685109130043,
    -0.20660801878927318,
    0.015019712610520534,
    -0.0719041153599492,
    0.2043125317509332,
    0.023417271835099653,
    -0.18997798051112605,
    -0.10697038241960508,
    0.5577778283659484
  ],
  "viewer_initial_pose": [
    0.6322896830675089,
    -0.36169896454635103,
    0.6851157802910678,
    -0.6990692285877195,
    -0.763027817742368,
    -0.44387309549620757,
    0.4698566437213055,
    -0.3144542709245074,
    0.13415780224907545,
    -0.8198478642149838,
    -0.5566427363361958,
    0.4296724330283282
  ],
  "method_id": "scaffold-gs",
  "renderer": {
    "scene_url": "https://huggingface.co/datasets/nerfbaselines/nerfbaselines-supplementary/resolve/main/scaffold-gs/mipnerf360-sparse/counter-n12_demo/scene.ksplat",
    "type": "3dgs"
  },
  "dataset": {
    "url": "https://huggingface.co/datasets/nerfbaselines/nerfbaselines-data/resolve/main/mipnerf360-sparse/counter-n12-nbv.json"
  }
}